{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura de Corpus Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "from time import time\n",
    "\n",
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in open('./Data/resumenes.txt'):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumenes = MyCorpus()\n",
    "# f = list(sentences)\n",
    "# f[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definicion de Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenamiento(sentences):\n",
    "        \n",
    "             \n",
    "        #Entrenamiento del modelo\n",
    "        cores = multiprocessing.cpu_count() #cuenta el nro de nucles de la pc\n",
    "\n",
    "        w2v = Word2Vec(size=300, #Dimensionalidad Palabras Vector\n",
    "               window=3, #Contexto, distancia entre palabras predichas\n",
    "               min_count=1, #Minimo de palabras a buscar\n",
    "               workers=cores, #En mi CPU\n",
    "               sg=1, #Usamos el Modelo SkipGram\n",
    "               hs=0, #Cero para negative sampling, castigo a neurona\n",
    "               negative=20, #Palabras irrelevantes para el muestreo negativo\n",
    "               ns_exponent=0, #Muestrea frecuencias por igual,\n",
    "               alpha=0.025, #Tasa de aprendizaje\n",
    "               min_alpha=0.0005, #Tasa que se reducira durante el train\n",
    "               seed=25, #Semilla generar hash para palabras\n",
    "               max_vocab_size=None, #Dependera de la maquina 10M -> 1GB\n",
    "               sample=5, #Reduccion para palabras con alta frecuencia\n",
    "               iter=150, #Epocas, valores altos sobreentreno )?\n",
    "               compute_loss=True #Muestra valor de perdida en el train\n",
    "              )\n",
    "        \n",
    "        t = time()\n",
    "        w2v.build_vocab(sentences, #Oraciones nuevas\n",
    "               #update=True, #Agregar nuevo vocabulario\n",
    "               progress_per=100000, #Palabras para procesar con antecipacion\n",
    "               min_count=1\n",
    "              ) # prepare the model vocabulary\n",
    "        print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "        t = time()\n",
    "        w2v.train(sentences, total_examples=w2v.corpus_count, epochs=w2v.iter ,report_delay=3)\n",
    "        print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "        \n",
    "        return w2v\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entramiento modelo 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.09 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sangeeky/anaconda3/envs/tesis/lib/python3.7/site-packages/ipykernel_launcher.py:32: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    }
   ],
   "source": [
    "#Modelo V1 Entramiento con sentence = resumen\n",
    "modelo1 = entrenamiento(resumenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(size=300, #Dimensionalidad Palabras Vector\n",
    "               window=3, #Contexto, distancia entre palabras predichas\n",
    "               min_count=1, #Minimo de palabras a buscar\n",
    "               workers=4, #En mi CPU\n",
    "               sg=1, #Usamos el Modelo SkipGram\n",
    "               hs=0, #Cero para negative sampling, castigo a neurona\n",
    "               negative=20, #Palabras irrelevantes para el muestreo negativo\n",
    "               ns_exponent=0, #Muestrea frecuencias por igual,\n",
    "               alpha=0.025, #Tasa de aprendizaje\n",
    "               min_alpha=0.0005, #Tasa que se reducira durante el train\n",
    "               seed=25, #Semilla generar hash para palabras\n",
    "               max_vocab_size=None, #Dependera de la maquina 10M -> 1GB\n",
    "               sample=5, #Reduccion para palabras con alta frecuencia\n",
    "               iter=150, #Epocas, valores altos sobreentreno )?\n",
    "               compute_loss=True #Muestra valor de perdida en el train\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.build_vocab(corpus_file='./Data/resumenes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabulario con Diccionario de freq\n",
    "w2v.build_vocab_from_freq(dictionary_resumenes)\n",
    "#Voacabulario con letras\n",
    "#w2v.build_vocab(setences, #Oraciones nuevas\n",
    "#                corpus_file=path #txt\n",
    "#                update=True, #Agregar nuevo vocabulario\n",
    "#                progress_per=100000 #Palabras para procesar con antecipacion\n",
    "#                 min_count=1\n",
    "#               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.train(sentences, total_examples=w2v.corpus_count, report_delay=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tesis] *",
   "language": "python",
   "name": "conda-env-tesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
